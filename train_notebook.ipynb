{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe313247",
   "metadata": {},
   "source": [
    "# RAFDB Metric Learning Training Notebook\n",
    "\n",
    "This notebook demonstrates how to train a metric learning model on RAFDB with:\n",
    "- Augmentation (crop, rotate, grayscale)\n",
    "- Triplet margin mining\n",
    "- MPerClassSampler for balanced batches\n",
    "- wandb logging for experiment tracking\n",
    "- Real-time triplet visualization\n",
    "\n",
    "**Key differences from main.py:**\n",
    "- Command-line arguments replaced with cell variables\n",
    "- Interactive execution with visual feedback\n",
    "- Better for debugging and experimentation\n",
    "- Visualizations display inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: Import Required Modules\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Add current directory to path for relative imports\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import wandb\n",
    "from pytorch_metric_learning.miners import TripletMarginMiner\n",
    "from pytorch_metric_learning.losses import TripletMarginLoss\n",
    "from pytorch_metric_learning.samplers import MPerClassSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import custom modules\n",
    "from dataset import train_val_split, getdataset_from_imagefolder\n",
    "from train import train_one_epoch\n",
    "from evalute import evaluate_one_epoch, final_eval\n",
    "from visualize import (\n",
    "    compute_embeddings_and_predictions,\n",
    "    compute_confusion_matrix,\n",
    "    visualize_confusion_matrix,\n",
    "    visualize_pca,\n",
    ")\n",
    "from model import EmbeddingModel, validate_model_and_augmentation\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6fbe1",
   "metadata": {},
   "source": [
    "## Configuration: Replace command-line arguments with variables\n",
    "\n",
    "Instead of parsing `sys.argv`, define parameters directly below. Modify these cells to match your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e53106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: Convert Command-Line Arguments to Variables\n",
    "# ============================================================================\n",
    "\n",
    "# Dataset arguments\n",
    "RAFDB_ROOT = \"/path/to/rafdb\"  # ← CHANGE THIS to your RAFDB path\n",
    "VAL_FRACTION = 0.2\n",
    "SEED = 42\n",
    "\n",
    "# Augmentation arguments\n",
    "NUM_AUGMENTATIONS = 2\n",
    "CROP_SCALE = (0.8, 1.0)\n",
    "ROTATION_DEGREES = 15\n",
    "GRAYSCALE_PROB = 0.3\n",
    "\n",
    "# Training arguments\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 28  # ← IMPORTANT: For RAFDB (7 classes), use batch_size = m * num_classes\n",
    "LEARNING_RATE = 1e-3\n",
    "LR_STEP = 10\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Model arguments\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "# Metric learning arguments\n",
    "MARGIN = 0.1\n",
    "METRIC = \"euclidean\"  # or \"cosine\"\n",
    "M_PER_CLASS = 4  # ← samples per class in each batch\n",
    "\n",
    "# Formula: batch_size should be <= m * num_classes\n",
    "# For RAFDB: batch_size <= 4 * 7 = 28\n",
    "# So with M_PER_CLASS=4 and 7 classes, max batch_size = 28\n",
    "# If you want larger batch_size, either:\n",
    "#   - Increase M_PER_CLASS (e.g., M_PER_CLASS=6 → batch_size <= 42)\n",
    "#   - Use smaller batch_size (e.g., batch_size=20)\n",
    "\n",
    "# Logging/output arguments\n",
    "USE_WANDB = False  # Set to True if you have wandb configured\n",
    "OUTPUT_DIR = \"./outputs\"\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  RAFDB_ROOT: {RAFDB_ROOT}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"  Use wandb: {USE_WANDB}\")\n",
    "print(f\"\\n  MPerClassSampler constraint:\")\n",
    "print(f\"  - M_PER_CLASS: {M_PER_CLASS}\")\n",
    "print(f\"  - Num classes (RAFDB): 7\")\n",
    "print(f\"  - Max batch_size: {M_PER_CLASS * 7}\")\n",
    "print(f\"  - Your batch_size: {BATCH_SIZE}\")\n",
    "if BATCH_SIZE > M_PER_CLASS * 7:\n",
    "    print(f\"  ⚠ WARNING: batch_size ({BATCH_SIZE}) > m*num_classes ({M_PER_CLASS * 7})\")\n",
    "    print(f\"     This will cause an AssertionError!\")\n",
    "else:\n",
    "    print(f\"  ✓ Valid configuration\")\n",
    "\n",
    "# Initialize wandb if enabled\n",
    "if USE_WANDB:\n",
    "    wandb.init(\n",
    "        project=\"rafdb-metric-learning\",\n",
    "        name=f\"batch={BATCH_SIZE}_m={M_PER_CLASS}_embed={EMBEDDING_DIM}\",\n",
    "        config={\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"num_epochs\": NUM_EPOCHS,\n",
    "            \"embedding_dim\": EMBEDDING_DIM,\n",
    "            \"margin\": MARGIN,\n",
    "            \"m_per_class\": M_PER_CLASS,\n",
    "            \"metric\": METRIC,\n",
    "            \"num_augmentations\": NUM_AUGMENTATIONS,\n",
    "        },\n",
    "    )\n",
    "    print(\"✓ wandb initialized\")\n",
    "else:\n",
    "    print(\"⚠ wandb disabled (set USE_WANDB=True to enable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b685d541",
   "metadata": {},
   "source": [
    "## Classes and Helper Functions\n",
    "\n",
    "Copy all class and function definitions from main.py. These are needed before executing the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ab6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: Class and Function Definitions\n",
    "# ============================================================================\n",
    "\n",
    "class RAFDBWithAugmentation(Dataset):\n",
    "    \"\"\"RAFDB dataset wrapper with controlled augmentation variance.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        split: str = \"train\",\n",
    "        num_augmentations: int = 2,\n",
    "        crop_scale: tuple = (0.8, 1.0),\n",
    "        rotation_degrees: int = 15,\n",
    "        grayscale_prob: float = 0.3,\n",
    "    ):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.num_augmentations = num_augmentations\n",
    "        self.crop_scale = crop_scale\n",
    "        self.rotation_degrees = rotation_degrees\n",
    "        self.grayscale_prob = grayscale_prob\n",
    "\n",
    "        split_dir = self.root / split\n",
    "        if not split_dir.exists():\n",
    "            raise FileNotFoundError(f\"RAFDB split directory not found: {split_dir}\")\n",
    "\n",
    "        self.base_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        self.augmentation_transforms = [\n",
    "            self._create_augmentation_transform()\n",
    "            for _ in range(num_augmentations)\n",
    "        ]\n",
    "\n",
    "        self.dataset = datasets.ImageFolder(\n",
    "            str(split_dir),\n",
    "            transform=None,\n",
    "        )\n",
    "\n",
    "    def _create_augmentation_transform(self):\n",
    "        transforms_list = [\n",
    "            transforms.RandomResizedCrop(224, scale=self.crop_scale),\n",
    "            transforms.RandomRotation(self.rotation_degrees),\n",
    "        ]\n",
    "\n",
    "        if self.grayscale_prob > 0:\n",
    "            transforms_list.append(\n",
    "                transforms.RandomGrayscale(p=self.grayscale_prob)\n",
    "            )\n",
    "\n",
    "        transforms_list.extend([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        return transforms.Compose(transforms_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        img_base = self.base_transform(img)\n",
    "        img_augmented = [\n",
    "            aug_transform(img) for aug_transform in self.augmentation_transforms\n",
    "        ]\n",
    "        all_views = torch.stack([img_base] + img_augmented, dim=0)\n",
    "        return all_views, label\n",
    "\n",
    "\n",
    "def collate_fn_with_augmentations(batch):\n",
    "    \"\"\"Custom collate to handle augmented views.\"\"\"\n",
    "    views_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for views, label in batch:\n",
    "        num_views = views.shape[0]\n",
    "        for v in range(num_views):\n",
    "            views_list.append(views[v])\n",
    "            labels_list.append(label)\n",
    "\n",
    "    images = torch.stack(views_list, dim=0)\n",
    "    labels = torch.tensor(labels_list, dtype=torch.long)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def visualize_triplet_mining(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    miner,\n",
    "    device: torch.device,\n",
    "    num_samples: int = 3,\n",
    "):\n",
    "    \"\"\"Visualize triplet mining results.\"\"\"\n",
    "    model.eval()\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"VISUALIZING TRIPLET MINING RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_images, batch_labels = next(iter(train_loader))\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        embeddings = model(batch_images)\n",
    "        anchor_idx, positive_idx, negative_idx = miner(embeddings, batch_labels)\n",
    "\n",
    "        if anchor_idx.numel() == 0:\n",
    "            print(\"⚠ Miner returned 0 triplets. Skipping visualization.\")\n",
    "            return\n",
    "\n",
    "        print(f\"✓ Mined {anchor_idx.numel()} triplets from batch of {batch_images.shape[0]} images\")\n",
    "\n",
    "        num_to_show = min(num_samples, anchor_idx.numel())\n",
    "\n",
    "        for triplet_num in range(num_to_show):\n",
    "            a_idx = anchor_idx[triplet_num].item()\n",
    "            p_idx = positive_idx[triplet_num].item()\n",
    "            n_idx = negative_idx[triplet_num].item()\n",
    "\n",
    "            a_label = batch_labels[a_idx].item()\n",
    "            p_label = batch_labels[p_idx].item()\n",
    "            n_label = batch_labels[n_idx].item()\n",
    "\n",
    "            a_img = batch_images[a_idx]\n",
    "            p_img = batch_images[p_idx]\n",
    "            n_img = batch_images[n_idx]\n",
    "\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "\n",
    "            a_img_vis = (a_img * std + mean).clamp(0, 1).cpu().permute(1, 2, 0).numpy()\n",
    "            p_img_vis = (p_img * std + mean).clamp(0, 1).cpu().permute(1, 2, 0).numpy()\n",
    "            n_img_vis = (n_img * std + mean).clamp(0, 1).cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "            fig = plt.figure(figsize=(14, 5))\n",
    "            gs = GridSpec(1, 3, figure=fig, wspace=0.3)\n",
    "\n",
    "            ax_a = fig.add_subplot(gs[0, 0])\n",
    "            ax_a.imshow(a_img_vis)\n",
    "            ax_a.set_title(f\"Anchor\\nLabel: {a_label}\", fontsize=12, fontweight=\"bold\", color=\"blue\")\n",
    "            ax_a.axis(\"off\")\n",
    "\n",
    "            ax_p = fig.add_subplot(gs[0, 1])\n",
    "            ax_p.imshow(p_img_vis)\n",
    "            status = \"✓ Same\" if p_label == a_label else \"✗ Wrong\"\n",
    "            ax_p.set_title(\n",
    "                f\"Positive\\nLabel: {p_label}\\n{status}\",\n",
    "                fontsize=12,\n",
    "                fontweight=\"bold\",\n",
    "                color=\"green\" if p_label == a_label else \"red\",\n",
    "            )\n",
    "            ax_p.axis(\"off\")\n",
    "\n",
    "            ax_n = fig.add_subplot(gs[0, 2])\n",
    "            ax_n.imshow(n_img_vis)\n",
    "            status = \"✓ Different\" if n_label != a_label else \"✗ Wrong\"\n",
    "            ax_n.set_title(\n",
    "                f\"Negative\\nLabel: {n_label}\\n{status}\",\n",
    "                fontsize=12,\n",
    "                fontweight=\"bold\",\n",
    "                color=\"green\" if n_label != a_label else \"red\",\n",
    "            )\n",
    "            ax_n.axis(\"off\")\n",
    "\n",
    "            fig.suptitle(\n",
    "                f\"Triplet {triplet_num + 1}/{num_to_show}\",\n",
    "                fontsize=14,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(\n",
    "                f\"  Triplet {triplet_num + 1}: \"\n",
    "                f\"Anchor(L={a_label}) -> Pos(L={p_label}) + Neg(L={n_label}) | \"\n",
    "                f\"Valid: {p_label == a_label and n_label != a_label}\"\n",
    "            )\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "print(\"✓ All classes and functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18122319",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Initialization\n",
    "\n",
    "Create output directory and load dataset with validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50facc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: Setup and Load Dataset\n",
    "# ============================================================================\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(OUTPUT_DIR)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"✓ Output directory: {output_dir}\")\n",
    "\n",
    "# Load RAFDB dataset\n",
    "print(\"\\nLoading RAFDB dataset...\")\n",
    "print(f\"  RAFDB root: {RAFDB_ROOT}\")\n",
    "print(f\"  Augmentations: {NUM_AUGMENTATIONS} views per sample\")\n",
    "print(f\"  Crop scale: {CROP_SCALE}\")\n",
    "print(f\"  Rotation: ±{ROTATION_DEGREES}°\")\n",
    "print(f\"  Grayscale prob: {GRAYSCALE_PROB}\")\n",
    "\n",
    "dataset = RAFDBWithAugmentation(\n",
    "    root=RAFDB_ROOT,\n",
    "    split=\"train\",\n",
    "    num_augmentations=NUM_AUGMENTATIONS,\n",
    "    crop_scale=CROP_SCALE,\n",
    "    rotation_degrees=ROTATION_DEGREES,\n",
    "    grayscale_prob=GRAYSCALE_PROB,\n",
    ")\n",
    "print(f\"✓ Dataset loaded: {len(dataset)} samples\")\n",
    "\n",
    "# Load test dataset\n",
    "print(\"\\nLoading test dataset...\")\n",
    "test_dataset = RAFDBWithAugmentation(\n",
    "    root=RAFDB_ROOT,\n",
    "    split=\"test\",\n",
    "    num_augmentations=NUM_AUGMENTATIONS,\n",
    "    crop_scale=CROP_SCALE,\n",
    "    rotation_degrees=ROTATION_DEGREES,\n",
    "    grayscale_prob=GRAYSCALE_PROB,\n",
    ")\n",
    "print(f\"✓ Test dataset loaded: {len(test_dataset)} samples\")\n",
    "\n",
    "# Validate augmentation and model\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 1: Validating augmentation and model...\")\n",
    "print(\"=\" * 70)\n",
    "temp_model = EmbeddingModel(\n",
    "    model_name=\"resnet50\",\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    pretrained=True,\n",
    ")\n",
    "if not validate_model_and_augmentation(temp_model, dataset, num_samples=4, device=DEVICE):\n",
    "    print(\"\\n❌ Validation failed. Exiting.\")\n",
    "    raise RuntimeError(\"Model validation failed\")\n",
    "del temp_model\n",
    "model = temp_model\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: Preparing train/val split...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Split into train/val\n",
    "train_ds, val_ds = train_val_split(\n",
    "    dataset, val_fraction=VAL_FRACTION, seed=SEED\n",
    ")\n",
    "print(f\"✓ Train samples: {len(train_ds)}\")\n",
    "print(f\"✓ Val samples: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46551c2c",
   "metadata": {},
   "source": [
    "## Step 2: Create DataLoaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create samplers and DataLoaders\n",
    "train_labels = torch.tensor([dataset.dataset.targets[i] for i in train_ds.indices])\n",
    "num_classes = len(torch.unique(train_labels))\n",
    "max_batch_size = M_PER_CLASS * num_classes\n",
    "\n",
    "print(f\"\\nDataLoader setup:\")\n",
    "print(f\"  Num unique classes in train set: {num_classes}\")\n",
    "print(f\"  M_PER_CLASS: {M_PER_CLASS}\")\n",
    "print(f\"  Max allowed batch_size: {max_batch_size}\")\n",
    "print(f\"  Requested batch_size: {BATCH_SIZE}\")\n",
    "\n",
    "if BATCH_SIZE > max_batch_size:\n",
    "    print(f\"\\n⚠ ERROR: batch_size ({BATCH_SIZE}) > m*num_classes ({max_batch_size})\")\n",
    "    print(f\"MPerClassSampler requires: batch_size <= m * (number of unique labels)\")\n",
    "    print(f\"\\nFix options:\")\n",
    "    print(f\"  1. Reduce batch_size to {max_batch_size} or less\")\n",
    "    print(f\"  2. Increase M_PER_CLASS (e.g., from {M_PER_CLASS} to {int(BATCH_SIZE/num_classes)+1})\")\n",
    "    print(f\"  3. Keep original batch_size but use a different sampler (not MPerClassSampler)\")\n",
    "    raise AssertionError(f\"batch_size ({BATCH_SIZE}) must be <= m*num_classes ({max_batch_size})\")\n",
    "\n",
    "sampler = MPerClassSampler(\n",
    "    train_labels,\n",
    "    m=M_PER_CLASS,\n",
    "    length_before_new_iter=len(train_ds),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn_with_augmentations,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn_with_augmentations,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn_with_augmentations,\n",
    ")\n",
    "\n",
    "print(f\"✓ Train DataLoader: {len(train_loader)} batches\")\n",
    "print(f\"✓ Val DataLoader: {len(val_loader)} batches\")\n",
    "print(f\"✓ Test DataLoader: {len(test_loader)} batches\")\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3: Initializing embedding model...\")\n",
    "print(\"=\" * 70)\n",
    "model = EmbeddingModel(\n",
    "    model_name=\"resnet18\",\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    pretrained=True,\n",
    "    normalize=True,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "print(\"✓ Model initialized:\")\n",
    "print(\"  - Backbone: ResNet18 (pretrained)\")\n",
    "print(f\"  - Embedding dim: {EMBEDDING_DIM}\")\n",
    "print(\"  - Normalization: Enabled (L2)\")\n",
    "print(f\"  - Device: {DEVICE}\")\n",
    "print(f\"  - Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Setup loss and optimizer\n",
    "loss_fn = TripletMarginLoss(margin=MARGIN, swap=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=LR_STEP, gamma=0.1\n",
    ")\n",
    "\n",
    "# Miner for hard triplet selection\n",
    "miner = TripletMarginMiner(\n",
    "    margin=MARGIN, type_of_triplets=\"semihard\", distance=None\n",
    ")\n",
    "\n",
    "print(\"✓ Loss, optimizer, and miner initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7974ad29",
   "metadata": {},
   "source": [
    "## Step 3: Visualize Triplet Mining\n",
    "\n",
    "Before training, verify that the miner is selecting correct triplets (same class as positive, different class as negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6929db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize triplet mining\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4A: Visualizing triplet mining...\")\n",
    "print(\"=\" * 70)\n",
    "visualize_triplet_mining(model, train_loader, miner, DEVICE, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5905ecd",
   "metadata": {},
   "source": [
    "## Step 4: Training Loop\n",
    "\n",
    "Run the training loop. You can interrupt this cell to stop training at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ebaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: Training Loop\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4B: Starting training...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_accuracy = 0.0\n",
    "history = {\"train_loss\": [], \"val_recall_1\": [], \"val_accuracy\": []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss = train_one_epoch(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        device=DEVICE,\n",
    "        miner=miner,\n",
    "        epoch=epoch,\n",
    "    )\n",
    "\n",
    "    # Validate\n",
    "    val_metrics = evaluate_one_epoch(\n",
    "        model=model,\n",
    "        val_loader=val_loader,\n",
    "        device=DEVICE,\n",
    "        metric=METRIC,\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Track history\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_recall_1\"].append(val_metrics.get(\"recall_at_1\", 0))\n",
    "    history[\"val_accuracy\"].append(val_metrics.get(\"accuracy\", 0))\n",
    "\n",
    "    # Print summary\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{NUM_EPOCHS} | \"\n",
    "        f\"Loss: {train_loss:.4f} | \"\n",
    "        f\"Recall@1: {val_metrics.get('recall_at_1', 0):.4f} | \"\n",
    "        f\"Accuracy: {val_metrics.get('accuracy', 0):.4f}\"\n",
    "    )\n",
    "\n",
    "    # Log to wandb\n",
    "    if USE_WANDB:\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_recall_at_1\": val_metrics.get(\"recall_at_1\", 0),\n",
    "            \"val_recall_at_5\": val_metrics.get(\"recall_at_5\", 0),\n",
    "            \"val_recall_at_10\": val_metrics.get(\"recall_at_10\", 0),\n",
    "            \"val_accuracy\": val_metrics.get(\"accuracy\", 0),\n",
    "        })\n",
    "\n",
    "    # Save best model based on validation accuracy\n",
    "    if val_metrics.get(\"accuracy\", 0) > best_accuracy:\n",
    "        best_accuracy = val_metrics[\"accuracy\"]\n",
    "        model_path = output_dir / \"best_model.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"  ✓ Saved best model\")\n",
    "        if USE_WANDB:\n",
    "            wandb.save(str(model_path))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Close wandb\n",
    "if USE_WANDB:\n",
    "    wandb.finish()\n",
    "    print(\"✓ wandb finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b35d0ec",
   "metadata": {},
   "source": [
    "## Step 5: Visualization and Analysis\n",
    "\n",
    "Plot training history and visualize embeddings using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(history[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history[\"val_recall_1\"], label=\"Recall@1\", marker=\"o\")\n",
    "axes[1].plot(history[\"val_accuracy\"], label=\"Accuracy\", marker=\"s\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Metric\")\n",
    "axes[1].set_title(\"Validation Metrics\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"training_history.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved training history to {output_dir / 'training_history.png'}\")\n",
    "\n",
    "# Final evaluation and visualization\n",
    "print(\"\\nFinal evaluation and visualization...\")\n",
    "embeddings, labels = compute_embeddings_and_predictions(\n",
    "    model, val_loader, device=DEVICE\n",
    ")\n",
    "\n",
    "cm = compute_confusion_matrix(embeddings, labels, metric=METRIC)\n",
    "visualize_confusion_matrix(\n",
    "    cm, output_path=str(output_dir / \"confusion_matrix.png\")\n",
    ")\n",
    "visualize_pca(embeddings, labels, output_path=str(output_dir / \"pca.png\"))\n",
    "\n",
    "# Final test evaluation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 6: Final evaluation on test set...\")\n",
    "print(\"=\" * 70)\n",
    "test_metrics = final_eval(\n",
    "    model, test_loader, device=DEVICE, metric=METRIC\n",
    ")\n",
    "print(f\"Test Recall@1: {test_metrics.get('recall_at_1', 0):.4f}\")\n",
    "print(f\"Test Recall@5: {test_metrics.get('recall_at_5', 0):.4f}\")\n",
    "print(f\"Test Recall@10: {test_metrics.get('recall_at_10', 0):.4f}\")\n",
    "print(f\"Test Accuracy: {test_metrics.get('accuracy', 0):.4f}\")\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({\n",
    "        \"test_recall_at_1\": test_metrics.get(\"recall_at_1\", 0),\n",
    "        \"test_recall_at_5\": test_metrics.get(\"recall_at_5\", 0),\n",
    "        \"test_recall_at_10\": test_metrics.get(\"recall_at_10\", 0),\n",
    "        \"test_accuracy\": test_metrics.get(\"accuracy\", 0),\n",
    "    })\n",
    "\n",
    "print(f\"\\n✓ All results saved to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
